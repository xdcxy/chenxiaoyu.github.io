<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  

  
  <title>中文文本数据处理 | Sakya Personal Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="1.中文文本挖掘预处理特点首先我们看看中文文本挖掘预处理和英文文本挖掘预处理相比的一些特殊点。 a).中文文本是没有像英文的单词空格那样隔开的，因此不能直接像英文一样可以直接用最简单的空格和标点符号完成分词。所以一般我们需要用分词算法来完成分词，在文本挖掘的分词原理中，中文的分词原理可以参考文本挖掘的分词原理，这里就不多说。 b).中文的编码不是utf8，而是unicode。这样会导致在分词的时候">
<meta name="keywords" content="practice">
<meta property="og:type" content="article">
<meta property="og:title" content="中文文本数据处理">
<meta property="og:url" content="http://yoursite.com/2019/01/15/text-processing/index.html">
<meta property="og:site_name" content="Sakya Personal Blog">
<meta property="og:description" content="1.中文文本挖掘预处理特点首先我们看看中文文本挖掘预处理和英文文本挖掘预处理相比的一些特殊点。 a).中文文本是没有像英文的单词空格那样隔开的，因此不能直接像英文一样可以直接用最简单的空格和标点符号完成分词。所以一般我们需要用分词算法来完成分词，在文本挖掘的分词原理中，中文的分词原理可以参考文本挖掘的分词原理，这里就不多说。 b).中文的编码不是utf8，而是unicode。这样会导致在分词的时候">
<meta property="og:locale" content="default">
<meta property="og:updated_time" content="2019-01-15T09:35:43.918Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="中文文本数据处理">
<meta name="twitter:description" content="1.中文文本挖掘预处理特点首先我们看看中文文本挖掘预处理和英文文本挖掘预处理相比的一些特殊点。 a).中文文本是没有像英文的单词空格那样隔开的，因此不能直接像英文一样可以直接用最简单的空格和标点符号完成分词。所以一般我们需要用分词算法来完成分词，在文本挖掘的分词原理中，中文的分词原理可以参考文本挖掘的分词原理，这里就不多说。 b).中文的编码不是utf8，而是unicode。这样会导致在分词的时候">
  
    <link rel="alternate" href="/atom.xml" title="Sakya Personal Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Sakya Personal Blog</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-text-processing" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/01/15/text-processing/" class="article-date">
  <time datetime="2019-01-15T06:25:32.000Z" itemprop="datePublished">2019-01-15</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      中文文本数据处理
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="1-中文文本挖掘预处理特点"><a href="#1-中文文本挖掘预处理特点" class="headerlink" title="1.中文文本挖掘预处理特点"></a>1.中文文本挖掘预处理特点</h1><p>首先我们看看中文文本挖掘预处理和英文文本挖掘预处理相比的一些特殊点。</p>
<p>a).中文文本是没有像英文的单词空格那样隔开的，因此不能直接像英文一样可以直接用最简单的空格和标点符号完成分词。所以一般我们需要用分词算法来完成分词，在文本挖掘的分词原理中，中文的分词原理可以参考<a href="http://www.cnblogs.com/pinard/p/6677078.html" target="_blank" rel="noopener">文本挖掘的分词原理</a>，这里就不多说。</p>
<p>b).中文的编码不是utf8，而是unicode。这样会导致在分词的时候，和英文相比，我们要处理编码的问题。<br><a id="more"></a></p>
<h1 id="2-处理中文编码问题"><a href="#2-处理中文编码问题" class="headerlink" title="2.处理中文编码问题"></a>2.处理中文编码问题</h1><p>由于Python2不支持unicode的处理，因此我们使用Python2做中文文本预处理时需要遵循的原则是，存储数据都用utf8，读出来进行中文相关处理时，使用GBK之类的中文编码，在下面一节的分词时，我们再用例子说明这个问题。</p>
<h1 id="3-中文分词"><a href="#3-中文分词" class="headerlink" title="3.中文分词"></a>3.中文分词</h1><p>常用的中文分词软件有很多，个人比较推荐结巴分词。安装也很简单，比如基于Python的，用”pip install jieba”就可以完成。下面我们就用例子来看看如何中文分词。</p>
<p>完整代码参见这个github: <a href="https://github.com/ljpzzz/machinelearning/blob/master/natural-language-processing/chinese_digging.ipynb" target="_blank" rel="noopener">https://github.com/ljpzzz/machinelearning/blob/master/natural-language-processing/chinese_digging.ipynb</a></p>
<p>首先我们准备了两段文本，这两段文本在两个文件中。两段文本的内容分别是nlp_test0.txt和nlp_test2.txt：</p>
<p>沙瑞金赞叹易学习的胸怀，是金山的百姓有福，可是这件事对李达康的触动很大。易学习又回忆起他们三人分开的前一晚，大家一起喝酒话别，易学习被降职到道口县当县长，王大路下海经商，李达康连连赔礼道歉，觉得对不起大家，他最对不起的是王大路，就和易学习一起给王大路凑了5万块钱，王大路自己东挪西撮了5万块，开始下海经商。没想到后来王大路竟然做得风生水起。沙瑞金觉得他们三人，在困难时期还能以沫相助，很不容易。</p>
<p>沙瑞金向毛娅打听他们家在京州的别墅，毛娅笑着说，王大路事业有成之后，要给欧阳菁和她公司的股权，她们没有要，王大路就在京州帝豪园买了三套别墅，可是李达康和易学习都不要，这些房子都在王大路的名下，欧阳菁好像去住过，毛娅不想去，她觉得房子太大很浪费，自己家住得就很踏实。</p>
<p>我们先讲文本从第一个文件中读取，并使用中文GBK编码，再调用结巴分词，最后把分词结果用uft8格式存在另一个文本nlp_test1.txt中。代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"></span><br><span class="line">import jieba</span><br><span class="line"></span><br><span class="line">with open(&apos;./nlp_test0.txt&apos;) as f:</span><br><span class="line">    document = f.read()</span><br><span class="line">    </span><br><span class="line">    document_decode = document.decode(&apos;GBK&apos;)</span><br><span class="line">    document_cut = jieba.cut(document_decode)</span><br><span class="line">    #print  &apos; &apos;.join(jieba_cut)  //如果打印结果，则分词效果消失，后面的result无法显示</span><br><span class="line">    result = &apos; &apos;.join(document_cut)</span><br><span class="line">    result = result.encode(&apos;utf-8&apos;)</span><br><span class="line">    with open(&apos;./nlp_test1.txt&apos;, &apos;w&apos;) as f2:</span><br><span class="line">        f2.write(result)</span><br><span class="line">f.close()</span><br><span class="line">f2.close()</span><br></pre></td></tr></table></figure>
<p>输出的文本内容如下：</p>
<p>沙 瑞金 赞叹 易 学习 的 胸怀 ， 是 金山 的 百姓 有福 ， 可是 这件 事对 李达康 的 触动 很大 。 易 学习 又 回忆起 他们 三人 分开 的 前一晚 ， 大家 一起 喝酒 话别 ， 易 学习 被 降职 到 道口 县当 县长 ， 王 大路 下海经商 ， 李达康 连连 赔礼道歉 ， 觉得 对不起 大家 ， 他 最 对不起 的 是 王 大路 ， 就 和 易 学习 一起 给 王 大路 凑 了 5 万块 钱 ， 王 大路 自己 东挪西撮 了 5 万块 ， 开始 下海经商 。 没想到 后来 王 大路 竟然 做 得 风生水 起 。 沙 瑞金 觉得 他们 三人 ， 在 困难 时期 还 能 以沫 相助 ， 很 不 容易 。</p>
<p>可以发现对于一些人名和地名，jieba处理的不好，不过我们可以帮jieba加入词汇如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">jieba.suggest_freq(&apos;沙瑞金&apos;, True)</span><br><span class="line">jieba.suggest_freq(&apos;易学习&apos;, True)</span><br><span class="line">jieba.suggest_freq(&apos;王大路&apos;, True)</span><br><span class="line">jieba.suggest_freq(&apos;京州&apos;, True)</span><br></pre></td></tr></table></figure>
<p>现在我们再来进行读文件，编码，分词，编码和写文件，代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">with open(&apos;./nlp_test0.txt&apos;) as f:</span><br><span class="line">    document = f.read()</span><br><span class="line">    </span><br><span class="line">    document_decode = document.decode(&apos;GBK&apos;)</span><br><span class="line">    document_cut = jieba.cut(document_decode)</span><br><span class="line">    #print  &apos; &apos;.join(jieba_cut)</span><br><span class="line">    result = &apos; &apos;.join(document_cut)</span><br><span class="line">    result = result.encode(&apos;utf-8&apos;)</span><br><span class="line">    with open(&apos;./nlp_test1.txt&apos;, &apos;w&apos;) as f2:</span><br><span class="line">        f2.write(result)</span><br><span class="line">f.close()</span><br><span class="line">f2.close()</span><br></pre></td></tr></table></figure>
<p>输出的文本内容如下：</p>
<p>沙瑞金 赞叹 易学习 的 胸怀 ， 是 金山 的 百姓 有福 ， 可是 这件 事对 李达康 的 触动 很大 。 易学习 又 回忆起 他们 三人 分开 的 前一晚 ， 大家 一起 喝酒 话别 ， 易学习 被 降职 到 道口 县当 县长 ， 王大路 下海经商 ， 李达康 连连 赔礼道歉 ， 觉得 对不起 大家 ， 他 最 对不起 的 是 王大路 ， 就 和 易学习 一起 给 王大路 凑 了 5 万块 钱 ， 王大路 自己 东挪西撮 了 5 万块 ， 开始 下海经商 。 没想到 后来 王大路 竟然 做 得 风生水 起 。 沙瑞金 觉得 他们 三人 ， 在 困难 时期 还 能 以沫 相助 ， 很 不 容易 。</p>
<p>基本已经可以满足要求。同样的方法我们对第二段文本nlp_test2.txt进行分词和写入文件nlp_test3.txt。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">with open(&apos;./nlp_test2.txt&apos;) as f:</span><br><span class="line">    document2 = f.read()</span><br><span class="line">    </span><br><span class="line">    document2_decode = document2.decode(&apos;GBK&apos;)</span><br><span class="line">    document2_cut = jieba.cut(document2_decode)</span><br><span class="line">    #print  &apos; &apos;.join(jieba_cut)</span><br><span class="line">    result = &apos; &apos;.join(document2_cut)</span><br><span class="line">    result = result.encode(&apos;utf-8&apos;)</span><br><span class="line">    with open(&apos;./nlp_test3.txt&apos;, &apos;w&apos;) as f2:</span><br><span class="line">        f2.write(result)</span><br><span class="line">f.close()</span><br><span class="line">f2.close()</span><br></pre></td></tr></table></figure>
<p>输出的文本内容如下：</p>
<p>沙瑞金 向 毛娅 打听 他们 家 在 京州 的 别墅 ， 毛娅 笑 着 说 ， 王大路 事业有成 之后 ， 要 给 欧阳 菁 和 她 公司 的 股权 ， 她们 没有 要 ， 王大路 就 在 京州 帝豪园 买 了 三套 别墅 ， 可是 李达康 和 易学习 都 不要 ， 这些 房子 都 在 王大路 的 名下 ， 欧阳 菁 好像 去 住 过 ， 毛娅 不想 去 ， 她 觉得 房子 太大 很 浪费 ， 自己 家住 得 就 很 踏实 。</p>
<p>可见分词效果还不错。</p>
<h1 id="4-引入停用词"><a href="#4-引入停用词" class="headerlink" title="4.引入停用词"></a>4.引入停用词</h1><p>在上面我们解析的文本中有很多无效的词，比如“着”，“和”，还有一些标点符号，这些我们不想在文本分析的时候引入，因此需要去掉，这些词就是停用词。常用的中文停用词表是1208个，<a href="http://files.cnblogs.com/files/pinard/stop_words.zip" target="_blank" rel="noopener">下载地址在这</a>。当然也有其他版本的停用词表，不过这个1208词版是我常用的。</p>
<p>在我们用scikit-learn做特征处理的时候，可以通过参数stop_words来引入一个数组作为停用词表。</p>
<p>现在我们将停用词表从文件读出，并切分成一个数组备用：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#从文件导入停用词表</span><br><span class="line">stpwrdpath = &quot;stop_words.txt&quot;</span><br><span class="line">stpwrd_dic = open(stpwrdpath, &apos;rb&apos;)</span><br><span class="line">stpwrd_content = stpwrd_dic.read()</span><br><span class="line">#将停用词表转换为list  </span><br><span class="line">stpwrdlst = stpwrd_content.splitlines()</span><br><span class="line">stpwrd_dic.close()</span><br></pre></td></tr></table></figure>
<h1 id="5-特征处理"><a href="#5-特征处理" class="headerlink" title="5.特征处理"></a>5.特征处理</h1><p>现在我们就可以用scikit-learn来对我们的文本特征进行处理了。有两种特征处理的方法，可以参考：<a href="http://www.cnblogs.com/pinard/p/6688348.html" target="_blank" rel="noopener">文本挖掘预处理之向量化与Hash Trick</a>。而向量化是最常用的方法，因为它可以接着进行TF-IDF的特征处理。这里我们就用scikit-learn的TfidfVectorizer类来进行TF-IDF特征处理。</p>
<p>TfidfVectorizer类可以帮助我们完成向量化，TF-IDF和标准化三步。当然，还可以帮我们处理停用词。</p>
<p>现在我们把上面分词好的文本载入内存：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">with open(&apos;./nlp_test1.txt&apos;) as f3:</span><br><span class="line">    res1 = f3.read()</span><br><span class="line">print res1</span><br><span class="line">with open(&apos;./nlp_test3.txt&apos;) as f4:</span><br><span class="line">    res2 = f4.read()</span><br><span class="line">print res2</span><br></pre></td></tr></table></figure></p>
<p>这里的输出还是我们上面分完词的文本。现在我们可以进行向量化，TF-IDF和标准化三步处理了。注意，这里我们引入了我们上面的停用词表。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.feature_extraction.text import TfidfVectorizer</span><br><span class="line">corpus = [res1,res2]</span><br><span class="line">vector = TfidfVectorizer(stop_words=stpwrdlst)</span><br><span class="line">tfidf = vector.fit_transform(corpus)</span><br><span class="line">print tfidf</span><br></pre></td></tr></table></figure>
<p>部分输出如下：</p>
<p>  (0, 44)   0.154467434933<br>  (0, 59)   0.108549295069<br>  (0, 39)   0.308934869866<br>  (0, 53)   0.108549295069<br>　　….<br>  (1, 27)   0.139891059658<br>  (1, 47)   0.139891059658<br>  (1, 30)   0.139891059658<br>  (1, 60)   0.139891059658</p>
<p>我们再来看看每次词和TF-IDF的对应关系：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">wordlist = vector.get_feature_names()#获取词袋模型中的所有词  </span><br><span class="line"># tf-idf矩阵 元素a[i][j]表示j词在i类文本中的tf-idf权重</span><br><span class="line">weightlist = tfidf.toarray()  </span><br><span class="line">#打印每类文本的tf-idf词语权重，第一个for遍历所有文本，第二个for便利某一类文本下的词语权重</span><br><span class="line">for i in range(len(weightlist)):  </span><br><span class="line">    print &quot;-------第&quot;,i,&quot;段文本的词语tf-idf权重------&quot;  </span><br><span class="line">    for j in range(len(wordlist)):  </span><br><span class="line">        print wordlist[j],weightlist[i][j]</span><br></pre></td></tr></table></figure>
<p>部分输出如下：</p>
<p>———-第 0 段文本的词语tf-idf权重———<br>一起 0.217098590137<br>万块 0.217098590137<br>三人 0.217098590137<br>三套 0.0<br>下海经商 0.217098590137<br>…..<br>———-第 1 段文本的词语tf-idf权重———<br>…..<br>李达康 0.0995336411066<br>欧阳 0.279782119316<br>毛娅 0.419673178975<br>沙瑞金 0.0995336411066<br>没想到 0.0<br>没有 0.139891059658<br>浪费 0.139891059658<br>王大路 0.29860092332<br>…..</p>
<h1 id="6-建立分析模型"><a href="#6-建立分析模型" class="headerlink" title="6.建立分析模型"></a>6.建立分析模型</h1><p>有了每段文本的TF-IDF的特征向量，我们就可以利用这些数据建立分类模型，或者聚类模型了，或者进行主题模型的分析。比如我们上面的两段文本，就可以是两个训练样本了。此时的分类聚类模型和之前讲的非自然语言处理的数据分析没有什么两样。因此对应的算法都可以直接使用。</p>
<h1 id="7-中文文本挖掘预处理总结"><a href="#7-中文文本挖掘预处理总结" class="headerlink" title="7.中文文本挖掘预处理总结"></a>7.中文文本挖掘预处理总结</h1><p>上面我们对中文文本挖掘预处理的过程做了一个总结，希望可以帮助到大家。需要注意的是这个流程主要针对一些常用的文本挖掘，并使用了词袋模型，对于某一些自然语言处理的需求则流程需要修改。比如我们涉及到词上下文关系的一些需求，此时不能使用词袋模型。而有时候我们对于特征的处理有自己的特殊需求，因此这个流程仅供自然语言处理入门者参考。</p>
<p>扩展链接：<a href="https://www.cnblogs.com/pinard/p/6756534.html" target="_blank" rel="noopener">英文文本挖掘预处理流程总结</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/01/15/text-processing/" data-id="cjvbs3o1e000u78ajnj4mf1q6" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/practice/">practice</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2019/02/26/python-hints/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          python开发备忘
        
      </div>
    </a>
  
  
    <a href="/2019/01/15/doc-classification-review/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">文本分类算法总结</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/algorithm/">algorithm</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/code/">code</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/paper/">paper</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/practice/">practice</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/share/">share</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/algorithm/" style="font-size: 15px;">algorithm</a> <a href="/tags/code/" style="font-size: 10px;">code</a> <a href="/tags/paper/" style="font-size: 15px;">paper</a> <a href="/tags/practice/" style="font-size: 20px;">practice</a> <a href="/tags/share/" style="font-size: 15px;">share</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/03/">March 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/02/">February 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/11/">November 2018</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2019/03/18/linux-hints/">linux开发备忘</a>
          </li>
        
          <li>
            <a href="/2019/02/26/python-hints/">python开发备忘</a>
          </li>
        
          <li>
            <a href="/2019/01/15/text-processing/">中文文本数据处理</a>
          </li>
        
          <li>
            <a href="/2019/01/15/doc-classification-review/">文本分类算法总结</a>
          </li>
        
          <li>
            <a href="/2018/12/28/tf-idf-based-IR/">基于TF-IDF的文本检索实践</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2019 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<!--script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script-->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

</body>
</html>