<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  

  
  <title>大规模multi-task learning学习通用句子表达(general purpose distributed sentence representation) | Sakya Personal Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="一、Paper TitleLearning General Purpose Distributed Sentence Representations via large scale multi-task Learning[ICLR2018]Yoshua Bengio挂名，主要机构MILA是蒙特利尔大学  二、Brief IntroductionMotivation1.这篇文章受Transfer L">
<meta name="keywords" content="paper">
<meta property="og:type" content="article">
<meta property="og:title" content="大规模multi-task learning学习通用句子表达(general purpose distributed sentence representation)">
<meta property="og:url" content="http://yoursite.com/2018/11/19/learn-general-sentence-representations/index.html">
<meta property="og:site_name" content="Sakya Personal Blog">
<meta property="og:description" content="一、Paper TitleLearning General Purpose Distributed Sentence Representations via large scale multi-task Learning[ICLR2018]Yoshua Bengio挂名，主要机构MILA是蒙特利尔大学  二、Brief IntroductionMotivation1.这篇文章受Transfer L">
<meta property="og:locale" content="default">
<meta property="og:updated_time" content="2018-11-19T13:06:25.923Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="大规模multi-task learning学习通用句子表达(general purpose distributed sentence representation)">
<meta name="twitter:description" content="一、Paper TitleLearning General Purpose Distributed Sentence Representations via large scale multi-task Learning[ICLR2018]Yoshua Bengio挂名，主要机构MILA是蒙特利尔大学  二、Brief IntroductionMotivation1.这篇文章受Transfer L">
  
    <link rel="alternate" href="/atom.xml" title="Sakya Personal Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Sakya Personal Blog</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-learn-general-sentence-representations" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/11/19/learn-general-sentence-representations/" class="article-date">
  <time datetime="2018-11-19T08:46:20.000Z" itemprop="datePublished">2018-11-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      大规模multi-task learning学习通用句子表达(general purpose distributed sentence representation)
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="一、Paper-Title"><a href="#一、Paper-Title" class="headerlink" title="一、Paper Title"></a>一、Paper Title</h1><p>Learning General Purpose Distributed Sentence Representations via large scale multi-task Learning[ICLR2018]<br>Yoshua Bengio挂名，主要机构MILA是蒙特利尔大学 </p>
<h1 id="二、Brief-Introduction"><a href="#二、Brief-Introduction" class="headerlink" title="二、Brief Introduction"></a>二、Brief Introduction</h1><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>1.这篇文章受Transfer Learning在NLP和CV中的成功启发。在例如image captioning和visual question answering任务中，经常在ImageNet上使用预训练得到的CNN去提取图片的表示。NLP中则用预训练的词向量。<br>2.虽然word embedding经常被用来初始化NLP的模型，但是word in context的学习却往往从头开始。这在大部分情况下是不可行的，特别是数据量较少的情况，不能很好地进行监督学习。于是本文的期望是有一个general purpose sentence representations可以对很多任务有所帮助，从而不需要仅仅实用了词向量，其他上下文的表示都要从头再学。<br>3.要在representation learning中有所进展，需要理解不同的网络模型建模的结果产生的inductive biases是什么样的，比如NMT模型被证明主要抓取形态学(morphology)和句法(syntactic)的属性。<br>4.要在diverse的任务中generalize,那么对句子从不同角度进行编码是十分重要的。利用不同模型方法(skip-thoughts,machine translation,natural language inference,constituency parsing)会得到不同inductive biases。</p>
<h2 id="Main-Methology"><a href="#Main-Methology" class="headerlink" title="Main Methology"></a>Main Methology</h2><p>1.采用one-to-many multi-task learning framework,其中encoder模块在不同任务中是共享的。这个框架设计基于的假设是从大量没什么关联的NLP任务中学习的句子表示更具普适性，能够更好地处理没有见过的任务，since this process encodes the inductive biases of multiple models。<br>2.目标是得到fixed-length distributed sentence representations。</p>
<h2 id="Experimental-Results"><a href="#Experimental-Results" class="headerlink" title="Experimental Results"></a>Experimental Results</h2><h1 id="三、Core-Idea"><a href="#三、Core-Idea" class="headerlink" title="三、Core Idea"></a>三、Core Idea</h1><p>这篇paperd的multi-task learning考虑了6个任务，其中5个都是Seqence-to-sequence learning。seq2seq模型是encoder-decoder模型的特殊情况，因为它的输入输出是序列化的。直接建模条件概率$P(\mathbf{y}|\mathbf{x})$。<br>所以下面先考虑单个的seq2seq模型<br>令$\mathbf{h_x}$表示input经过encoder处理后的固定长度的向量表示。<br>encoder采用bidirectional GRU,decoder用unidirectional GRU,公式如下：</p>
<script type="math/tex; mode=display">
\mathbf{r_t} = \sigma(\mathbf{W_r}x_t + \mathbf{U_rh_t-1} + \mathbf{C_rh_x})</script><script type="math/tex; mode=display">
\mathbf{z_t} = \sigma(\mathbf{W_z}x_t + \mathbf{U_zh_t-1} + \mathbf{C_zh_x})</script><script type="math/tex; mode=display">
\tilde{\mathbf{h_t}} = tanh(\mathbf{W_d}x_t + \mathbf{U_dr_t \odot h_t-1} + \mathbf{C_dh_x})</script><script type="math/tex; mode=display">
\mathbf{h_{t+1}} = (1 - \mathbf{z_t}) \odot \mathbf{h_{t-1}} + \mathbf{z_t} \odot \tilde{\mathbf{h_t}}</script><p>接下来是这篇paper关心的：multi-task seq2seq model.<br>原文引用：In this work, we consider a one-to-many model since it lends itself naturally to the idea of combining inductive biases from different training objectives.</p>
<h2 id="Training-Objectives"><a href="#Training-Objectives" class="headerlink" title="Training Objectives"></a>Training Objectives</h2><p>最终选用了如下的训练目标来学习general-purpose sentence embeddings.</p>
<h3 id="Skip-thought-vectors"><a href="#Skip-thought-vectors" class="headerlink" title="Skip-thought vectors"></a>Skip-thought vectors</h3><p>把skip-gram word embedding model扩展成句子embedding，用的语料是contiguous sentences组成的BookCorpus。学习目标是根据当前句子预测后一句和前一句是什么。有人证明仅仅预测后一句而不用预测前一句可以达到差不多的效果。<br>参考论文：《Skip-thought vectors》</p>
<h3 id="Neural-Machine-Translation"><a href="#Neural-Machine-Translation" class="headerlink" title="Neural Machine Translation"></a>Neural Machine Translation</h3><p>用450万的英德语料和4000万的英法语料训练seq2seq模型，利用multiple target languages来训练表示在别的论文中已经证明了可以带来提升。<br>参考论文：《Multi-task learning for multiple language translation》</p>
<h3 id="Constituency-Parsing-linearized-parse-tree-construction"><a href="#Constituency-Parsing-linearized-parse-tree-construction" class="headerlink" title="Constituency Parsing (linearized parse tree construction)"></a>Constituency Parsing (linearized parse tree construction)</h3><p>Oriol Vinyals证明了seq2seq model来做constituency parsing是可行的。encoder的输入是自己，输出是其线性解析书(linearized parse tree)<br>参考论文：《Grammar as a foreign language》</p>
<h3 id="Natural-Language-Inference"><a href="#Natural-Language-Inference" class="headerlink" title="Natural Language Inference"></a>Natural Language Inference</h3><p>和上面不同的是，这是三分类问题：entailment，contradiction, neutral。用encoder对premise和hypothesis编码成固定长度的vector u和v。然后把feature [u;v;|u - v|;u * v] 输入MLP训练分类器。训练数据是SNLI和MultiNLI，数据规模100万。<br>参考论文：《Supervised learning of universal sentence representations from natural language inference data》</p>
<h1 id="四、Related-Work"><a href="#四、Related-Work" class="headerlink" title="四、Related Work"></a>四、Related Work</h1><h1 id="五、Appendix"><a href="#五、Appendix" class="headerlink" title="五、Appendix"></a>五、Appendix</h1>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/11/19/learn-general-sentence-representations/" data-id="cjoo89rza000321aj62s1lj13" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/paper/">paper</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2018/11/14/semantic-matching-Toturial/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">semantic_matching_Toturial</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/paper/">paper</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/practice/">practice</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/share/">share</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/paper/" style="font-size: 10px;">paper</a> <a href="/tags/practice/" style="font-size: 10px;">practice</a> <a href="/tags/share/" style="font-size: 10px;">share</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/11/">November 2018</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2018/11/19/learn-general-sentence-representations/">大规模multi-task learning学习通用句子表达(general purpose distributed sentence representation)</a>
          </li>
        
          <li>
            <a href="/2018/11/14/semantic-matching-Toturial/">semantic_matching_Toturial</a>
          </li>
        
          <li>
            <a href="/2018/11/14/environment/">必备环境配置</a>
          </li>
        
          <li>
            <a href="/2018/11/12/hello-world/">Hello World</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2018 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<!--script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script-->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

</body>
</html>