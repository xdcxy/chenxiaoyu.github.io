<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  

  
  <title>大规模multi-task learning学习通用句子表达(general purpose distributed sentence representation) | Sakya Personal Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="一、Paper TitleLearning General Purpose Distributed Sentence Representations via large scale multi-task Learning[ICLR2018]Yoshua Bengio挂名，主要机构MILA是蒙特利尔大学">
<meta name="keywords" content="paper">
<meta property="og:type" content="article">
<meta property="og:title" content="大规模multi-task learning学习通用句子表达(general purpose distributed sentence representation)">
<meta property="og:url" content="http://yoursite.com/2018/11/19/learn-general-sentence-representations/index.html">
<meta property="og:site_name" content="Sakya Personal Blog">
<meta property="og:description" content="一、Paper TitleLearning General Purpose Distributed Sentence Representations via large scale multi-task Learning[ICLR2018]Yoshua Bengio挂名，主要机构MILA是蒙特利尔大学">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://piiuo16g4.bkt.clouddn.com/sr_pic1.png">
<meta property="og:image" content="http://piiuo16g4.bkt.clouddn.com/sr_pic2.png">
<meta property="og:image" content="http://piiuo16g4.bkt.clouddn.com/sr_pic3.png">
<meta property="og:image" content="http://piiuo16g4.bkt.clouddn.com/sr_pic5.jpg">
<meta property="og:image" content="http://piiuo16g4.bkt.clouddn.com/sr_pic4.png">
<meta property="og:updated_time" content="2018-11-21T02:28:09.832Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="大规模multi-task learning学习通用句子表达(general purpose distributed sentence representation)">
<meta name="twitter:description" content="一、Paper TitleLearning General Purpose Distributed Sentence Representations via large scale multi-task Learning[ICLR2018]Yoshua Bengio挂名，主要机构MILA是蒙特利尔大学">
<meta name="twitter:image" content="http://piiuo16g4.bkt.clouddn.com/sr_pic1.png">
  
    <link rel="alternate" href="/atom.xml" title="Sakya Personal Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Sakya Personal Blog</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-learn-general-sentence-representations" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/11/19/learn-general-sentence-representations/" class="article-date">
  <time datetime="2018-11-19T08:46:20.000Z" itemprop="datePublished">2018-11-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      大规模multi-task learning学习通用句子表达(general purpose distributed sentence representation)
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="一、Paper-Title"><a href="#一、Paper-Title" class="headerlink" title="一、Paper Title"></a>一、Paper Title</h1><p>Learning General Purpose Distributed Sentence Representations via large scale multi-task Learning[ICLR2018]<br>Yoshua Bengio挂名，主要机构MILA是蒙特利尔大学<br><a id="more"></a></p>
<h1 id="二、Brief-Introduction"><a href="#二、Brief-Introduction" class="headerlink" title="二、Brief Introduction"></a>二、Brief Introduction</h1><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>1.这篇文章受Transfer Learning在NLP和CV中的成功启发。在例如image captioning和visual question answering任务中，经常在ImageNet上使用预训练得到的CNN去提取图片的表示。NLP中则用预训练的词向量。<br>2.虽然word embedding经常被用来初始化NLP的模型，但是word in context的学习却往往从头开始。这在大部分情况下是不可行的，特别是数据量较少的情况，不能很好地进行监督学习。于是本文的期望是有一个general purpose sentence representations可以对很多任务有所帮助，从而不需要仅仅实用了词向量，其他上下文的表示都要从头再学。<br>3.要在representation learning中有所进展，需要理解不同的网络模型建模的结果产生的inductive biases是什么样的，比如NMT模型被证明主要抓取形态学(morphology)和句法(syntactic)的属性。<br>4.要在diverse的任务中generalize,那么对句子从不同角度进行编码是十分重要的。利用不同模型方法(skip-thoughts,machine translation,natural language inference,constituency parsing)会得到不同inductive biases。</p>
<h2 id="Main-Methology"><a href="#Main-Methology" class="headerlink" title="Main Methology"></a>Main Methology</h2><p>1.采用one-to-many multi-task learning framework,其中encoder模块在不同任务中是共享的。这个框架设计基于的假设是从大量没什么关联的NLP任务中学习的句子表示更具普适性，能够更好地处理没有见过的任务，since this process encodes the inductive biases of multiple models。<br>2.目标是得到fixed-length distributed sentence representations。</p>
<h2 id="Experimental-Results"><a href="#Experimental-Results" class="headerlink" title="Experimental Results"></a>Experimental Results</h2><p>这篇文章主要从三个角度衡量了模型的效果，这三个角度都是参考的以下论文：《Skip-thought vectors》、《Learning distributed representations of sentences from unlabelled data》、《Supervised learning of universal sentence representations from natural language inference data》<br>1.将学习到的通用的句子表示用于没参与训练的文本分类任务的简单分类器的feature<br>下面的图列出了10个分类任务上模型的效果， 这篇paper使用的是简单逻辑回归加上通用句子表示，主要对比的对象是Infersent，最后一列都列出了相对于这个baseline的平均提升。整体上task越多效果越好。<br><img src="http://piiuo16g4.bkt.clouddn.com/sr_pic1.png" alt="pic1"><br>其中(MR, CR, SUBJ &amp; MPQA)这四个是情感分类任务，相对提升在1.1%至2%，TREC上提升较大，相比于infersent 6%，CNN-LSTM 2%<br>2.这种transfer learning的方法在low-resource的任务中表现如何</p>
<p>下面这个图所示的是在通用句子表示的基础上加上一个MLP就能够比肩甚至超过很多运用attention机制的复杂模型。而且有个很重要的点是所用的数据占训练集很小的比例就可以达到不错的效果。<br><img src="http://piiuo16g4.bkt.clouddn.com/sr_pic2.png" alt="pic2"><br>3.评估这个框架学习到的word embedding效果如何。词向量的评价标准参考论文：《Community evaluation and exchange of word vectors at wordvectors》、《Evaluation of word vector representations by subspace alignment》<br>这个图展示的就是multi-task learning 学习到的词向量在标准的评价指标里是可以与Glove，Word2vec，fasttext等相提并论的。<br><img src="http://piiuo16g4.bkt.clouddn.com/sr_pic3.png" alt="pic3"></p>
<p>另外地，作者深入研究了这些句子embedding有没有像这两篇参考论文《Fine-grained analysis<br>of sentence embeddings using auxiliary prediction tasks》、《Does string-based neural mt learn source syntax?》提到的可以infer一些sentence characteristics和syntactic properties。实验发现句法属性通过增加multi-lingual NMT来更好的encode。仅从NLI任务学习表示可以encode syntax但是融入到multi-task的框架里没有增强这一信号。类似地，作者发现诸如长度，单词顺序这些sentence characteristics可以通过加入parsing更好地encode。实验见下图：<br><img src="http://piiuo16g4.bkt.clouddn.com/sr_pic5.jpg" alt="pic4"></p>
<p>这篇文章参考skip-thought里面的方式对句子表示做了个可视化，应用t-SNE技术进行投影，效果如下：<br><img src="http://piiuo16g4.bkt.clouddn.com/sr_pic4.png" alt="pic5"><br>在image-caption retrieval任务中，这篇文章的sentence embedding是超过skip-thoughts,并且能和Infersent差不多的。直接计算两个句子embedding的余弦相似度可以不错地衡量他们的semantic textual similarity。</p>
<h1 id="三、Core-Idea"><a href="#三、Core-Idea" class="headerlink" title="三、Core Idea"></a>三、Core Idea</h1><p>这篇paperd的multi-task learning考虑了6个任务，其中5个都是Seqence-to-sequence learning。seq2seq模型是encoder-decoder模型的特殊情况，因为它的输入输出是序列化的。直接建模条件概率$P(\mathbf{y}|\mathbf{x})$。<br>所以下面先考虑单个的seq2seq模型<br>令$\mathbf{h_x}$表示input经过encoder处理后的固定长度的向量表示。<br>encoder采用bidirectional GRU,decoder用unidirectional GRU,公式如下：</p>
<script type="math/tex; mode=display">
\mathbf{r_t} = \sigma(\mathbf{W_r}x_t + \mathbf{U_rh_t-1} + \mathbf{C_rh_x})</script><script type="math/tex; mode=display">
\mathbf{z_t} = \sigma(\mathbf{W_z}x_t + \mathbf{U_zh_t-1} + \mathbf{C_zh_x})</script><script type="math/tex; mode=display">
\tilde{\mathbf{h_t}} = tanh(\mathbf{W_d}x_t + \mathbf{U_dr_t \odot h_t-1} + \mathbf{C_dh_x})</script><script type="math/tex; mode=display">
\mathbf{h_{t+1}} = (1 - \mathbf{z_t}) \odot \mathbf{h_{t-1}} + \mathbf{z_t} \odot \tilde{\mathbf{h_t}}</script><p>接下来是这篇paper关心的：multi-task seq2seq model.<br>原文引用：In this work, we consider a one-to-many model since it lends itself naturally to the idea of combining inductive biases from different training objectives.</p>
<h2 id="Training-Objectives"><a href="#Training-Objectives" class="headerlink" title="Training Objectives"></a>Training Objectives</h2><p>最终选用了如下的训练目标来学习general-purpose sentence embeddings.</p>
<h3 id="Skip-thought-vectors"><a href="#Skip-thought-vectors" class="headerlink" title="Skip-thought vectors"></a>Skip-thought vectors</h3><p>把skip-gram word embedding model扩展成句子embedding，用的语料是contiguous sentences组成的BookCorpus。学习目标是根据当前句子预测后一句和前一句是什么。有人证明仅仅预测后一句而不用预测前一句可以达到差不多的效果。<br>参考论文：《Skip-thought vectors》</p>
<h3 id="Neural-Machine-Translation"><a href="#Neural-Machine-Translation" class="headerlink" title="Neural Machine Translation"></a>Neural Machine Translation</h3><p>用450万的英德语料和4000万的英法语料训练seq2seq模型，利用multiple target languages来训练表示在别的论文中已经证明了可以带来提升。<br>参考论文：《Multi-task learning for multiple language translation》</p>
<h3 id="Constituency-Parsing-linearized-parse-tree-construction"><a href="#Constituency-Parsing-linearized-parse-tree-construction" class="headerlink" title="Constituency Parsing (linearized parse tree construction)"></a>Constituency Parsing (linearized parse tree construction)</h3><p>Oriol Vinyals证明了seq2seq model来做constituency parsing是可行的。encoder的输入是自己，输出是其线性解析书(linearized parse tree)<br>参考论文：《Grammar as a foreign language》</p>
<h3 id="Natural-Language-Inference"><a href="#Natural-Language-Inference" class="headerlink" title="Natural Language Inference"></a>Natural Language Inference</h3><p>和上面不同的是，这是三分类问题：entailment，contradiction, neutral。用encoder对premise和hypothesis编码成固定长度的vector u和v。然后把feature [u;v;|u - v|;u * v] 输入MLP训练分类器。训练数据是SNLI和MultiNLI，数据规模100万。<br>参考论文：《Supervised learning of universal sentence representations from natural language inference data》</p>
<h1 id="四、Related-Work"><a href="#四、Related-Work" class="headerlink" title="四、Related Work"></a>四、Related Work</h1><p>这篇文章其实就是很多文章的一个集成，融入进了一个multi-task的学习框架之中。<br>主要借鉴的工作是skip-thought vectors和Supervised learning of universal sentence representations from natural language inference data</p>
<h1 id="五、Appendix"><a href="#五、Appendix" class="headerlink" title="五、Appendix"></a>五、Appendix</h1><h3 id="Multi-task-模型细节："><a href="#Multi-task-模型细节：" class="headerlink" title="Multi-task 模型细节："></a>Multi-task 模型细节：</h3><p><strong>+STN +Fr +De</strong> : The sentence representation hx is the concatenation of the final hidden vectors from a forward GRU with 1500-dimensional hidden vectors and a bidirectional GRU, also with 1500-dimensional hidden vectors.<br><strong>+STN +Fr +De +NLI</strong> : The sentence representation hx is the concatenation of the final hidden vectors from a bidirectional GRU with 1500-dimensional hidden vectors and another bidirectional GRU with 1500-dimensional hidden vectors trained without NLI.<br><strong>+STN +Fr +De +NLI +L</strong> : The sentence representation hx is the concatenation of the final hidden vectors from a bidirectional GRU with 2048-dimensional hidden vectors and another bidirectional GRU with 2048-dimensional hidden vectors trained without NLI.<br><strong>+STN +Fr +De +NLI +L +STP</strong> : The sentence representation hx is the concatenation of the fi- nal hidden vectors from a bidirectional GRU with 2048-dimensional hidden vectors and another bidirectional GRU with 2048-dimensional hidden vectors trained without STP.<br><strong>+STN +Fr +De +NLI +2L +STP</strong> : The sentence representation hx is the concatenation of the final hidden vectors from a 2-layer bidirectional GRU with 2048-dimensional hidden vectors and a 1-layer bidirectional GRU with 2048-dimensional hidden vectors trained without STP.<br><strong>+STN +Fr +De +NLI +L +STP +Par</strong> : The sentence representation hx is the concatenation of the final hidden vectors from a bidirectional GRU with 2048-dimensional hidden vectors and another bidirectional GRU with 2048-dimensional hidden vectors trained without Par.</p>
<h3 id="Evaluation-Tasks描述"><a href="#Evaluation-Tasks描述" class="headerlink" title="Evaluation Tasks描述"></a>Evaluation Tasks描述</h3><p>Related work里面对评价sentence embedding的任务有过具体的描述，这里做个汇总：</p>
<ul>
<li><p>文本分类<br>sentiment classification on movie reviews(MR),product reviews(CR) and Stanford sentiment(SST),question type classification (TREC),subjectivity/objectivity classification (SUBJ),opinion polarity (MPQA)。<br>训练都是用逻辑回归+10-fold validation to tune L2 weight penalty,评价标注都是accuracy。</p>
</li>
<li><p>paraphrase identification<br>pairwise text classification tasks。Dataset is Microsoft Research Paraphrase Corpus (MRPC) corpus。这是一个二分类的问题，判断两个句子是否是paraphrase，评价标注是accuracy和F1 score。</p>
</li>
<li><p>Entailment and semantic relatedness<br>这是为了测试相似句子是否它们的embedding也是相似的。所以在SICK relatedness任务上(SICK-R)训练一个线性分类型来给两个句子给出1-5的相似性评价，评价标注是Pearson correlation。利用同样的dataset的entailment label(SICK-E)来做二分类任务,评价标注是分类accuracy。</p>
</li>
<li><p>Semantic Textual Similarity<br>只用两个句子表示的余弦相似度作为依据。使用的是similarity textual similarity (STS) benchmark tasks from 2012-2016 (STS12, STS13, STS14, STS15, STS16, STSB)评价指标是Pearson correlation。</p>
</li>
<li><p>Image caption retireval<br>这是一个典型的排序任务。数据集来自MSCOCO。图像特征的抽取用预训练好的110层ResNet。评价指标是Recall@K和median K across 5 different splits of the data。</p>
</li>
<li><p>Quora duplicate question classification<br>这其实就是个二分类任务，判断两个问句是否重复。</p>
</li>
<li><p>sentence characteristics &amp; syntax</p>
<ul>
<li>sentence characteristics such as length, word content and word order<br>length task是把句子长度分成了8个等级，做8-way的classification。content task是给定一个句子的embedding和word的embedding拼接起来，判断词是否包含在句子中。order task是content task的扩展，输入是句子embedding分别和其中两个单词embedding拼接，判断第一个单词是在另一个前面还是后面出现的。</li>
<li>syntactic properties such as active/passive, tense and the top syntactic sequence (TSS) from the parse tree of a sentence<br>passive和tense task是给定sentence representation的二分类问题。前一个目标是判断句子是以主动还是被动的语气写出的，后一个是判断句子是过去式还是不是。The top sequence(TSS)是20-way classification包括19个最频繁的syntactic sequences和1个miscellaneous class。</li>
</ul>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/11/19/learn-general-sentence-representations/" data-id="cjoqjtkd20007vxajzrt27566" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/paper/">paper</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2018/11/14/semantic-matching-Toturial/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">semantic_matching_Toturial</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/paper/">paper</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/practice/">practice</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/share/">share</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/paper/" style="font-size: 10px;">paper</a> <a href="/tags/practice/" style="font-size: 10px;">practice</a> <a href="/tags/share/" style="font-size: 10px;">share</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/11/">November 2018</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2018/11/19/learn-general-sentence-representations/">大规模multi-task learning学习通用句子表达(general purpose distributed sentence representation)</a>
          </li>
        
          <li>
            <a href="/2018/11/14/semantic-matching-Toturial/">semantic_matching_Toturial</a>
          </li>
        
          <li>
            <a href="/2018/11/14/environment/">必备环境配置</a>
          </li>
        
          <li>
            <a href="/2018/11/12/hello-world/">Hello World</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2018 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<!--script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script-->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

</body>
</html>